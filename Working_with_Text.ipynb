{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Working with Text.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAw8EC9rhJbh"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['weather'].map({\n",
        "    \"Sunny\": 0,\n",
        "    \"Rainy\": 1,\n",
        "    \"Cloudy\":2\n",
        "})"
      ],
      "metadata": {
        "id": "8IRxjGTZjFa2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "data['weather'] = le.fit_transform(data['weather'])"
      ],
      "metadata": {
        "id": "jLGlSMF7jP73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for column in [\"weather\",\"feature_2\",'feature_3']:\n",
        "  data[column] = le.fit_transform(data[column])"
      ],
      "metadata": {
        "id": "XdJcB909lN5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.get_dummies(data, columns=\"weather\")"
      ],
      "metadata": {
        "id": "-0uYi358k3hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.get_dummies(data)"
      ],
      "metadata": {
        "id": "jfBvHGoRliK_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame(\n",
        "            data=[\"What a match\", \"Seen the news?\", \"Seen the match?\", \"What! What news?\", \"What a holiday!\", \"Going to France\"], \n",
        "            columns=['text']\n",
        "            )"
      ],
      "metadata": {
        "id": "W-cZLixA0hl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "features = vectorizer.fit_transform(data['text']).toarray()\n",
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amaorCtx0dya",
        "outputId": "d0775b25-4825-4a14-8335-360daa39911d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.7640961 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.64510243],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
              "        0.57735027, 0.57735027, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
              "        0.57735027, 0.57735027, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.50957052,\n",
              "        0.        , 0.        , 0.        , 0.8604289 ],\n",
              "       [0.        , 0.        , 0.82219037, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.56921261],\n",
              "       [0.57735027, 0.57735027, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.57735027, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "feature_counts = vectorizer.fit_transform(data['text']).toarray()\n",
        "\n",
        "vectorizer = TfidfTransformer()\n",
        "features = vectorizer.fit_transform(feature_counts).toarray()\n",
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8llefHMy1Ti8",
        "outputId": "3935a455-26a9-4b15-a3c6-ce76d8d42e18"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , 0.7640961 , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.64510243],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.57735027,\n",
              "        0.57735027, 0.57735027, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.57735027, 0.        ,\n",
              "        0.57735027, 0.57735027, 0.        , 0.        ],\n",
              "       [0.        , 0.        , 0.        , 0.        , 0.50957052,\n",
              "        0.        , 0.        , 0.        , 0.8604289 ],\n",
              "       [0.        , 0.        , 0.82219037, 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.        , 0.56921261],\n",
              "       [0.57735027, 0.57735027, 0.        , 0.        , 0.        ,\n",
              "        0.        , 0.        , 0.57735027, 0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer(stop_words=\"english\")\n",
        "features = vectorizer.fit_transform(data['text']).toarray()\n",
        "features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHPjrakOnnHl",
        "outputId": "93cb6111-6f1f-4440-aa1f-75b9341706ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 0, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 1, 1],\n",
              "       [0, 0, 0, 1, 0, 1],\n",
              "       [0, 0, 0, 0, 1, 0],\n",
              "       [0, 0, 1, 0, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].str.lower()\n",
        "data['text'] = data['text'].str.upper()\n",
        "data['text'] = data['text'].str.capitalize()"
      ],
      "metadata": {
        "id": "UnXqtZ2uqkh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['text'] = data['text'].str.replace('[^\\w\\s]','')"
      ],
      "metadata": {
        "id": "jJc7ifo5-ryj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "punct = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
        "def remove_punctuation(text):\n",
        "  for char in text:\n",
        "    if char in punct:\n",
        "      text = text.replace(char,'')\n",
        "  return text\n",
        "\n",
        "data['text'] = data['text'].apply(remove_punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbtTSp6V-0Ev",
        "outputId": "12530186-72f5-4306-b878-b70a8d64a2e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0       What a match\n",
              "1      Seen the news\n",
              "2     Seen the match\n",
              "3     What What news\n",
              "4     What a holiday\n",
              "5    Going to France\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AFmLG11eDfKx",
        "outputId": "a546dbd3-1c8a-4d52-d68f-f6b1c9bc3b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk # do this only once\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords') # do this only once\n",
        "nltk.download('punkt') # do this only once\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  clean_text = []\n",
        "  for word in word_tokenize(text):\n",
        "    if not word in stopwords.words():\n",
        "      clean_text.append(word)\n",
        "  return ' '.join(clean_text)\n",
        "\n",
        "data['text'] = data['text'].apply(remove_stopwords)"
      ],
      "metadata": {
        "id": "lSbxpgiSCtv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "data = pd.DataFrame(\n",
        "            data=[\"What a match\", \"Sen the news?\", \"Seen the matsh?\", \"Wat! What news?\", \"What a howliday!\", \"Going to France\"], \n",
        "            columns=['text']\n",
        "            )"
      ],
      "metadata": {
        "id": "NkxsnWfhIi0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install textblob\n",
        "from textblob import TextBlob\n",
        "\n",
        "texts = \"|\".join(data['text'].values)\n",
        "blob = TextBlob(texts)\n",
        "data['text'] = blob.correct().split('|')"
      ],
      "metadata": {
        "id": "3VefOa6DHW2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install autocorrect\n",
        "from autocorrect import Speller\n",
        "spell = Speller(lang='en')\n",
        "data['text'] = data['text'].apply(lambda x: spell(x))"
      ],
      "metadata": {
        "id": "mAJKrdMgImXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "ps =PorterStemmer()\n",
        "for w in e_words:Â²\n",
        "    rootWord=ps.stem(w)"
      ],
      "metadata": {
        "id": "h0rvwFGMJ1qd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet') # do this only once\n",
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "e_words= [\"wait\", \"waiting\", \"waited\", \"waits\"]\n",
        "for w in e_words:\n",
        "  print(wordnet_lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Swx5uyK_KRRG",
        "outputId": "278ec776-493c-4b4e-fcfc-2124d9b72e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "Lemma for wait is wait\n",
            "Lemma for waiting is waiting\n",
            "Lemma for waited is waited\n",
            "Lemma for waits is wait\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mapping = {\n",
        "    \"omg\": \"oh my god\",\n",
        "    \"wanna\": \"want to\",\n",
        "    \"bf\": \"boyfriend\",\n",
        "    \"gf\": \"girlfriend\"\n",
        "}\n",
        "for slang in mapping:\n",
        "  data['text'].str.replace(slang,mapping[slang])"
      ],
      "metadata": {
        "id": "mVm38s-lXTRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install fuzzywuzzy\n",
        "from fuzzywuzzy import fuzz\n",
        "fuzz.ratio(\"juuuuuusstt\", \"just\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkItaD26ZreX",
        "outputId": "29194bf6-faa3-488e-9211-3c1aeaa1dc8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "53"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install textdistance\n",
        "import textdistance\n",
        "textdistance.hamming('juuuuuusstt',\n",
        "                     'just')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu5PyDT9aJIR",
        "outputId": "7bf0f803-3d91-41c6-8fda-ee27e9110a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    }
  ]
}